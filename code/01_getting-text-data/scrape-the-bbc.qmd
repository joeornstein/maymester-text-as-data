---
title: "Webscrape the BBC"
format: pdf
editor: visual

message: false
warning: false
---

## Load Packages

```{r}
library(tidyverse)
library(rvest)
```

## Scrape the Page

```{r}
# Three steps to scrape a webpage:
# 1. Read html from site/page
page <- read_html('https://www.bbc.com/news/world-middle-east-64062900')
# 2. Get the elements on that page that you want
paragraphs <- html_elements(page, 'p')
# 3. From html elements convert into text
text <- html_text2(paragraphs)
```

## Print that text (tidily)

This text should overflow across the PDF margins. Not pretty.

```{r}
# let's paste all the paragraphs together
text <- paste(text, collapse = ' ')
text
```

We can solve this with the `str_wrap()` function from `tidyverse` , which inserts carriage returns ('\\n') to convert the text into paragraphs, and the `cat()` function, which prints the text.

```{r}
text |> 
  str_wrap() |> 
  cat()
```

## Once we have the text, let's tidy it and count up the words

```{r}
# first, let's *tokenize* the text, splitting it up into unigrams (words)
library(tidyverse)
library(tidytext)

# second, put the text into a dataframe
d <- tibble(text)

# then, use the unnest_tokens() function from tidytext to tokenize
# this function takes three inputs
# 1: the dataframe with the text you want to tokenize
# 2: the name of the column with the text data
# 3: the name you want to call the tokenized text column
d <- unnest_tokens(d, input = 'text', output = 'word')



```

## Some preliminary text analysis

```{r}
# frequency distribution of all words
d |> 
  count(word) |> 
  arrange(-n)

# frequency distribution of words, removing stop words like "the" and "of"
d |> 
  anti_join(get_stopwords(language = 'en', source = 'smart')) |> 
  count(word) |> 
  arrange(-n)


```

## Visualizing Frequency Distributions

We can visualize that frequency distribution in a number of ways.

```{r}
# bar chart of the 30 most common words
d |> 
  anti_join(get_stopwords(language = 'en', source = 'smart')) |> 
  count(word) |> 
  arrange(-n) |> 
  # just keep the first 30 rows (30 most common words)
  slice_head(n = 30) |> 
  # reorder the word variable as a factor, in order of word frequency
  mutate(word = fct_reorder(word, n)) |> 
  ggplot(mapping = aes(x=n, y=word)) +
  geom_col()
```

```{r}
#| eval: false

# word clouds are also cool (but won't render to pdf)
library(wordcloud2)
d |> 
  anti_join(get_stopwords(language = 'en', source = 'smart')) |> 
  count(word) |> 
  arrange(-n) |> 
  wordcloud2()
```

## Sentiment Analysis

If I want to know if this article is happy or sad, one approach is to count up the number of happy words vs the number of sad words. (Positive vs. negative sentiment).

```{r}

# take our tokenized dataset and merge it with the sentiment lexicon
d |> 
  # only keep words from the BBC article if they appear in the sentiment lexicon
  inner_join(get_sentiments(lexicon = 'bing'))

# take our tokenized dataset and merge it with the sentiment lexicon
d |> 
  # only keep words from the BBC article if they appear in the sentiment lexicon
  inner_join(get_sentiments(lexicon = 'bing')) |> 
  # count up positive and negative sentiment words
  count(sentiment)

```
