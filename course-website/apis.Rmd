---
title: "Application Programming Interfaces"
description: |
  When webscraping is too difficult and/or impolite
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
```

In the [webscraping tutorial](webscraping.html), we harvested text directly from the HTML code of a webpage. This can be pretty laborious, so fortunately, there are some websites that provide an easier path to collecting their data, called an Application Programming Interface (API). Generally, when an API is available, using it is the easiest and [most polite](https://joeornstein.github.io/text-as-data/webscraping.html#being-polite) way to harvest your text data. On this page, we'll introduce ourselves to APIs by collecting bill text from the US Congress and video transcripts from YouTube.

## Congress API

Suppose we wanted to collect the text of bills introduced in the US Congress. As an example, let's start the full text with H.R.1 from the 115th Congress (The Tax Cuts and Jobs Act) available [here](https://www.congress.gov/bill/115th-congress/house-bill/1/text). We can access that page through our web browser, but if I try to read the HTML in `R`...

```{r, eval = FALSE}
library(tidyverse)
library(rvest)

page <- read_html("https://www.congress.gov/bill/115th-congress/house-bill/1/text")
```

![](img/error-403.png)

...I get a foreboding "HTTP Error 403 - Forbidden" message. It turns out the US Federal Government does not take kindly to bots scraping their webpages.

Fortunately, the good people at congress.gov provide an API that we can use instead. An API is, in essence, a special web address that returns data instead of a webpage. For congress.gov, these web addresses all begin with `https://api.congress.gov/v3/`. But to access them, we'll need an **API key**, a unique password that identifies each user. You can sign up for one at the top of the [documentation page](https://gpo.congress.gov/).

Once you have your API key, you will include it as part of the web address you use to access information. Per the [API documentation](https://gpo.congress.gov/#/bill/bill_text), you can retrieve the text of bills with a web address in the following format:

```
https://api.congress.gov/v3/bill/{congress}/{billType}/{billNumber}/text?api_key={INSERT_KEY}
```

Wherever you see curly braces {}, replace them with values for the bill you want. In our example, `{congress} = 115`, `{billType} = hr`, and `{billNumber} = 1`. Rather than accessing this web address through the browser, we will ask `R` to read the data directly.

### Step 1: Keep Your API Key Safe

You may be tempted to copy-paste your API key directly into your `R` script. Avoid this temptation. It's best practice to keep things like passwords and API keys saved in a separate location, not hard-coded into your scripts. That way, if you share you code (like I'm doing now), you don't accidentally reveal your secrets. I'm keeping my API key in a text file called `congress-api-key.txt`. The first step is to read it into memory.

```{r}
api_key <- read_file('congress-api-key.txt')
```

### Step 2: Get the data from the API

We'll use the `glue` package to format the web addresses:

```{r}
library(glue)

congress <- 115
billType <- 'hr'
billNumber <- 1

glue('/bill/{congress}/{billType}/{billNumber}/text')

url <- glue('https://api.congress.gov/v3/bill/{congress}/{billType}/{billNumber}/text?api_key={api_key}')
```

Once we've formatted the web address, we can use the `httr` package to get data from the API.

```{r}
library(httr)
d <- GET(url)
```

### Step 3: Convert to content from JSON to an R object

The object `d` is a JSON object. Fortunately, we don't have to know what that is or how to read it. We can just use the `jsonlite` package to convert it into an `R` object.

```{r}
library(jsonlite)

# take the content attribute from d
content <- d$content |>
  # convert from unicode to English characters
  rawToChar() |>
  # convert the JSON format to an R object
  fromJSON()
```

Now we have a list called `content`. Sadly, this is not yet the content we want. The wrinkle here is that there are 8 versions of the text of this bill, following the amendment process from when it was first introduced in the House to when it was finally signed into law. The `content$textVersions` object tells us where to find each version of the text. 

```{r}
content$textVersions
```

### Step 4: Get The Bill Text

Let's get the text for the latest version of the bill (the version that became law).

```{r}
mostRecent <- content$textVersions |> 
  # keep the row with the most recent date
  slice_max(date, n = 1) |> 
  # pull the formats column
  pull(formats)

mostRecent

# get the URL for the most recent Formatted Text
textURL <- mostRecent[[1]] |> 
  filter(type == 'Formatted Text') |> 
  pull(url)

# read the text
text <- read_file(textURL)
```

Finally! The object `text` contains the entire text of the bill. Printing it in its entirety would be too long, but here's a snippet:

```{r}
text |> 
  substr(1,1000) |> 
  cat()
```

### Steps 1 through 4 in a Function

And that's the workflow for retrieving the full text of a bill from the Congress.gov API. If we ever plan to use that workflow again, it would be wise to put it all into a **function**. That way we're not copy-pasting large blocks of code every time we want to get the text of a new bill.

```{r}
get_bill_text <- function(congress, billType, billNumber){
  
  # Step 1: Get the API Key
  api_key <- read_file('congress-api-key.txt')
  
  # Step 2: Read the data from the API
  url <- glue('https://api.congress.gov/v3/bill/{congress}/{billType}/{billNumber}/text?api_key={api_key}')
  
  d <- GET(url)
  
  # Step 3: Convert the JSON object to an R list
  content <- d$content |>
    rawToChar() |>
    fromJSON()
  
  # Step 4: Get the most recent bill text
  mostRecent <- content$textVersions |>
    slice_max(date, n = 1) |>
    pull(formats)

  textURL <- mostRecent[[1]] |> 
    filter(type == 'Formatted Text') |> 
    pull(url)

  text <- read_file(textURL)
  
  # Return the text object
  return(text)
}
```

Does the function work? If so, it should get the same text object we created before:

```{r}
text == get_bill_text(congress = 115, billType = 'hr', billNumber = 1)
```


## YouTube API



[youtube-transcript-api](https://pypi.org/project/youtube-transcript-api/)

For `R` users, the [`reticulate`](https://rstudio.github.io/reticulate/) package is a convenient way to run Python code and return outputs as `R` objects. I'll demonstrate here how we can use that package to get transcripts from the YouTube API.

### Setup



```{r, echo = TRUE, eval = FALSE}
install.packages('reticulate')
reticulate::install_miniconda()
```

#### For Mac Users

If you are a Mac user, you should enter the following command to create a Python "virtual environment".

```{r, eval=FALSE}
reticulate::conda_create('myenv')
```

Then restart your R session, then enter the following command to use the virtual environment you created.

```{r, eval = FALSE}
reticulate::use_condaenv('myenv')
```

### Install the Python package

Regardless of your operating system, you want to install the `youtube-transcript-api` Python module, using the following function from `reticulate`.

```{r, eval=FALSE}
reticulate::py_install('youtube-transcript-api')
```



## Example Workflow

## Practice Problems

1. Get the full text of the [Patient Protection and Affordable Care Act (2010)](https://www.congress.gov/bill/111th-congress/house-bill/3590).

2. Using the `get_bill_text()` function we created, create a dataframe where each row is a bill, and include columns for `congress`, `billType`, `billNumber`, and `full_text`. Collect the full text for at least 10 bills.

## Further Reading

- Christopher Kennedy's [`congress`](http://christophertkenny.com/congress/) package, which provides a tidy `R` interface to the congress.gov API. 