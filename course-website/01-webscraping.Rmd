---
title: "Webscraping Tutorial"
description: |
  What to do when you just want text but the website where it lives is trying to sell you prescription medications or something.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

The central difficulty we face scraping text data from the web is that web pages are never *just* text. They come with a whole bunch of other junk to make the text look pretty. That junk is written in HTML (Hypertext Markup Language) code, and our first task as researchers is to separate the plain text we want from all the HTML code that's making it look pretty. 

For example, suppose for some reason I wanted to know what Tucker Carlson said on his television program on April 20, 2022. The transcript is [here](https://www.foxnews.com/transcript/tucker-the-us-is-looking-at-a-grim-economic-picture), but it's cluttered. There are graphics, ads, pictures, links to other pages, fonts, and a bunch of other things we don't need for our research. Fortunately, the plain text of the transcript is hiding in the page's HTML code, if we know where to look.

## The `rvest` package

As of writing (May 2022), the most user-friendly `R` package for getting text data from web pages is [`rvest`](https://rvest.tidyverse.org/index.html) (read that name like "harvest", as in harvesting data).

Let's begin by loading that package.

```{r}
library(tidyverse)
library(rvest)
```

### Reading HTML

To read the HTML from a web page, we can use the `read_html()` function, just like we would read a data file from our computer. Just supply it with the web page's URL.

```{r}
page <- read_html('https://www.foxnews.com/transcript/tucker-the-us-is-looking-at-a-grim-economic-picture')
```

### Get the elements we want

- Every HTML page is divided into sections by tags. If you can identify the tag you want, you're golden.
- ScraperTool comes in handy!