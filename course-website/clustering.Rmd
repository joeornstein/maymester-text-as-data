---
title: "Clustering"
description: |
  For when we don't really know what we're looking for in our data and just want the computer to tell us what it sees.
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Broadly speaking, we can divide the approaches for modeling text data into two camps: supervised learning and unsupervised learning. [Supervised learning](sentiment-analysis.html) approaches tend to be the most familiar to social scientists -- there is some outcome we'd like to predict, so we fit a function of observable covariates to try and predict it. In the context of text as data, this means we have a set of labeled documents, and we fit a model to see how well we can predict the labels (e.g. predicting the authorship of the [Federalist Papers](federalist-papers.html)).

Unsupervised learning, by comparison, is less about prediction and more about *discovery*. You start with a set of *unlabeled* documents, and ask the computer to see if it can find a sensible way to organize them. Are there patterns of language that distinguish one set of documents from others? What words can help identify a cluster of documents, by appearing within them more frequently than one would expect by chance? These sorts of approaches, which include both clustering and [topic models](LDA.html), require a healthy dose of human judgment to derive meaningful insights, and they often serve as the first stage of a research agenda that moves from discovery to explanation, prediction, and inference.

## K-means Clustering

Chapter 12 of @grimmerTextDataNew2021 introduces the dataset of Congressional press releases that \@grimmerRepresentationalStyleCongress2013 explores in his study of representational style. Using a k-means clustering model, he developed a set of categories to describe these ways that members of Congress communicate with their constituents, discovering new categories that were previously understudied by political scientists. The full dataset is available [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/14596), and I've included the press releases from Senator Lautenberg on the course repository. Let's load and tidy the data, representing each press release as a bag of word stems.

```{r}
library(tidyverse)
library(tidytext)
library(SnowballC)

load('data/press-releases/lautenberg-press-releases.RData')

tidy_press_releases <- df |>
  # remove the preamble common to each press release
  mutate(text = str_replace_all(text,
                                pattern = '     Senator Frank R  Lautenberg                                                                                                                      Press Release        of        Senator Lautenberg                                                                                ',
                                replacement = '')) |>
  # tokenize to the word level
  unnest_tokens(input = 'text',
                output = 'word') |>
  # remove stop words
  anti_join(get_stopwords()) |>
  # remove numerals
  filter(str_detect(word, '[0-9]', negate = TRUE)) |>
  # create word stems
  mutate(word_stem = wordStem(word)) |>
  filter(word_stem != '') |> 
  # count up bag of word stems
  count(id, word_stem) |> 
  # compute term frequency
  bind_tf_idf(term = 'word_stem',
              document = 'id',
              n = 'n') |>
  filter(!is.na(tf_idf))

tidy_press_releases
```

Next, we'll convert that tidy dataframe into a document-term matrix.

```{r}
# create document-term matrix
lautenberg_dtm <- cast_dtm(data = tidy_press_releases,
                           document = 'id',
                           term = 'word_stem',
                           value = 'tf')
lautenberg_dtm
```

The k-means clustering algorithm searches for a set of $k$ centroids that yield the smallest sum of squared distances between the observations and their nearest centroid. If each document is represented by a vector of term frequencies, then k-means produces $k$ sets of documents that have the most similar usages of words.[^1]

[^1]: Allison Horst has a delightful illustrated explanation of how the algorithm works on [this page](https://www.tidymodels.org/learn/statistics/k-means/).

```{r}
set.seed(42)

km <- kmeans(x = lautenberg_dtm,
             centers = 4,
             nstart = 100)

table(km$cluster)
```

Making sense of this algorithm's output is tricky. Sure, we simplified the problem a bit. We started with 558 documents each represented by a 7,073-dimensional vector, and now we have 4 document *clusters* represented by a 7,073-dimensional vector.

```{r, echo = FALSE}
knitr::include_graphics('img/thanks.gif')
```

So...what do we do with those?

One of the most common ways to interpret k-means clusters is to generate a list of the most distinctive words from each cluster. Then we can look at which words show up more frequently in one cluster than any other, and use that information to assign labels to the clusters.

```{r}
# function to find the words that are most overrepresented in the cluster mean for a given cluster
get_top_words <- function(centers, cluster_of_interest, n = 10){
  (centers[cluster_of_interest,] - colMeans(centers[-cluster_of_interest,])) |>
    sort(decreasing = TRUE) |>
    head(n)
}

# cluster 1 (security)
get_top_words(km$centers, 1)

# cluster 2 (legislation / senate business)
get_top_words(km$centers, 2)

# cluster 3 ("partisan taunting")
get_top_words(km$centers, 3)

# cluster 4 (credit claiming for New Jersey projects)
get_top_words(km$centers, 4)
```
