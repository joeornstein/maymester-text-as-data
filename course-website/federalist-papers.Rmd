---
title: "Federalist Paper Authorship"
description: |
  Modeling the bag of words.
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

A great entry point for modeling text as data is the now classic problem of guessing who wrote 15 disputed Federalist Papers [@mostellerInferenceDisputedAuthorship1964]. To do so, we'll look at the distribution of words that Hamilton, Madison, and Jay tend to write and try to detect their stylistic patterns in the disputed texts. We'll use two approaches, outlined in @grimmerTextDataNew2021 Chapters 6 and 7: the multinomial model and the vector space model.

## Multinomial Model

To motivate this model, suppose that each author owns a literal bag of words. When they are writing, they randomly draw each word from the bag, replacing it when they're done writing it down. Alexander Hamilton's bag of words has a *lot* of the word "upon", while James Madison's has relatively more "whilst".

```{r}
library(tidyverse)
library(tidytext)
library(wordcloud2)

tidy_federalist <- corpus::federalist |> 
  # tokenize to the word level
  unnest_tokens(input = 'text',
                output = 'word')

# keep only a selection of stop words
interesting_words <- c('although', 'always', 
                     'commonly', 'consequently',
                     'considerable', 'heretofore', 
                     'upon', 'whilst')

tidy_federalist <-filter(tidy_federalist,
                         word %in% interesting_words)

tidy_federalist |> 
  filter(author == 'Hamilton') |> 
  count(word) |> 
  wordcloud2()

tidy_federalist |> 
  filter(author == 'Hamilton') |> 
  count(word) |> 
  wordcloud2()
```

Let's consider Federalist Paper No. 18, counting the frequency of "upon", "whilst", and the other interesting stop words listed above.[^1]

[^1]: This list was inspired by Chapter 5 of @imaiQuantitativeSocialScience2017. More on how to identify discriminating words in a later section.

```{r}
tidy_federalist |> 
  filter(name == 'Federalist No. 18') |> 
  count(word)
```

What's the likelihood that this set of word counts would have been generated by random draws from Hamilton's bag of words, compared to Madison's or Jay's? (What's the chance that Hamilton would have written a paper with so few "upon"s?) To estimate, we'll first compute the word vectors for each author and for the disputed paper.

```{r}
# get the frequencies of the interesting words in each author's corpus
bags_of_words <- tidy_federalist |>
  filter(author %in% c('Hamilton', 'Madison', 'Jay')) |>
  # convert these to factors so count() doesn't drop the zero counts
  mutate(author = factor(author),
         word = factor(word)) |> 
  count(author, word, .drop = FALSE) |>
  # sort the words in alphabetical order
  arrange(author, word)

# pull the vectors for Hamilton, Madison, and Jay
hamilton_vector <- bags_of_words |>
  filter(author == 'Hamilton') |>
  pull(n) |>
  # add names to make the vector more readable
  set_names(interesting_words)

hamilton_vector

madison_vector <- bags_of_words |>
  filter(author == 'Madison') |>
  pull(n) |>
  # add names to make the vector more readable
  set_names(interesting_words)

madison_vector

jay_vector <- bags_of_words |>
  filter(author == 'Jay') |>
  pull(n) |>
  # add names to make the vector more readable
  set_names(interesting_words)

jay_vector

# now get the vector for Federalist No. 18
fed18_vector <- tidy_federalist |>
  mutate(word = factor(word)) |> 
  filter(name == 'Federalist No. 18') |>
  count(word, .drop = FALSE) |>
  pull(n) |>
  # add names to make the vector more readable
  set_names(interesting_words)

fed18_vector
```

Next we'll use the `dmultinom()` function to estimate the likelihood that `fed18_vector` would have been drawn from each of the authors' bags of words.

```{r}
dmultinom(x = fed18_vector,
          prob = hamilton_vector)

dmultinom(x = fed18_vector,
          prob = madison_vector)

dmultinom(x = fed18_vector,
          prob = jay_vector)
```

The computation above makes a very strong assumption about Jay: because he never uses the word "whilst" in any of his Federalist papers, we assume that he would *never* *ever* use the word whilst in another paper. We can do better by regularizing our estimates, adding a small positive number to each vector (Laplace smoothing). to encode the possibility that Jay might someday use the word "whilst", even if we've never seen him do it.

```{r}
hamilton_likelihood <- dmultinom(x = fed18_vector,
                                 prob = hamilton_vector + 0.1)

madison_likelihood <- dmultinom(x = fed18_vector,
                                prob = madison_vector + 0.1)

jay_likelihood <- dmultinom(x = fed18_vector,
                            prob = jay_vector + 0.1)

# likelihood ratios
madison_likelihood / hamilton_likelihood
madison_likelihood / jay_likelihood
```

Since this paper is roughly 28 times more likely to have been generated from Madison's bag of words over Hamilton's (and roughly 100 times more likely than Jay's), we can conclude with some degree of confidence that he was the author.

## Vector Space Model

Another way to model the bag of words is to think of each set of word counts as a multidimensional vector. The angle between two such vectors gives us a sense of the two documents' similarity to one another. If the angle is zero, then both documents have the exact same mix of words (though maybe one document is longer than the other). If the angle is 90 degrees, then the two documents are *orthogonal* -- as different a mix of words as they possibly could be.

Cosine similarity captures this idea, because the cosine of 90 degrees is zero, and the cosine of 0 degrees is 1. Let's compute the cosine similarity between Madison, Hamilton, and Jay's known writings with the term vector from the disputed Federalist No. 18.

```{r}

```

## Validation, Validation, Validation
