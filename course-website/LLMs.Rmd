---
title: "Large Language Models"
description: |
  A dumb auto-complete model -- trained on the entire Internet -- can accomplish a remarkable number of tasks if you ask it nicely.
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Large language models (LLMs) have transformed the way computer scientists approach natural language processing over the past decade. These models, trained to predict the next word in a sequence, make use of a massive quantity of text data from the Internet and digitized books. In this tutorial, I will hand-wave over exactly *how* LLMs perform this task, focusing instead on how to adapt such models for use in social science applications. If you are interested in what's going on under the hood, I highly recommend the following two blog posts for not-too-technical introductions:

-   [The GPT-3 Architecture, on a Napkin](https://dugas.ch/artificial_curiosity/GPT_architecture.html)

-   [Large language models, explained with a minimum of math and jargon](https://www.understandingai.org/p/large-language-models-explained-with)

In a nutshell, LLMs represent words using [embeddings](word-embeddings.html), and are trained to predict the most likely next word in a sequence using a model architecture called the **transformer** [@vaswaniAttentionAllYou2017]. A powerful feature of this type of model is that the embeddings representing the input sequence are iteratively updated based on which words appear nearby (a process called **self-attention**). This allows an LLM to flexibly represent words based on their context, which is useful in cases where the same word can mean different things in different contexts. For example, an LLM will represent the word "bill" differently depending on whether it appears in the phrase "sign the bill", "foot the bill", or "Hillary and Bill".

What's remarkable is that if we have a model that is sufficiently good at predicting the next word in a sequence, we can *adapt* it to perform all sorts of text-as-data tasks, by creating a prompt that converts our desired task into a next-word prediction problem [@ornsteinHowTrainYour2025]. To show you how, we will use my `promptr` R package to create LLM prompts and submit them to OpenAI's language models.[^1] Let's start by getting the R package set up.

[^1]: Like many others, I have qualms about using proprietary, closed-source models for scientific research [@spirlingWhyOpensourceGenerative2023]. But as of summer 2025 the ease-of-use and capabilities of OpenAI's models are sufficiently beyond those of similar open-source models that it makes sense to start here.

## Setting Up The `promptr` Package

The package is currently available on CRAN. You can install with the following line of code:

```{r, eval = FALSE}
install.packages('promptr')
```

Next, you will need an account with OpenAI. You can sign up for one [here](https://platform.openai.com/signup), after which you will need to generate an API key [here](https://platform.openai.com/account/api-keys). I recommend adding this [API key](APIs.html) as a variable in your operating system environment called `OPENAI_API_KEY`; that way you won't risk leaking it by hard-coding it into your `R` scripts. The `promptr` package will automatically look for your API key under that variable name, and will prompt you to enter the API key manually if it can't find one there. The easiest way to do so is by copy-pasting your API key into the following line of code:

```
promptr::openai_api_key('<YOUR KEY GOES HERE>', install = TRUE)
```

Once you've done this you may need to restart your R session. Once you've done that, we can begin.

## Completing Prompts

The workhorse function of the `promptr` package is `complete_prompt()`. This function submits a sequence of words (the prompt) to the OpenAI API, and returns a dataframe with the five highest-probability next word predictions and their associated probabilities.[^2]

[^2]: Note that the default model used by `complete_prompt()` is "gpt-3.5-turbo-instruct", essentially the model underlying the original ChatGPT. You can prompt different model variants using the `model` argument.

```{r, echo = TRUE, eval = FALSE}
library(promptr)

complete_prompt(prompt = 'My favorite food is')
```

```{r, echo = FALSE, eval = FALSE}
# NOTE: Throughout I'm saving the GPT-3 outputs to file so it's not re-running every time I knit the website
library(promptr)

food1 <- complete_prompt(prompt = 'My favorite food is')

save(food1, file = 'data/gpt-3-outputs/food1.RData')
```

```{r, echo = FALSE, eval = TRUE}
library(promptr)

load('data/gpt-3-outputs/food1.RData')

food1
```

If you prefer the model to auto-regressively generate sequences of text instead of outputting the next-word probabilities, set the `max_tokens` argument greater than 1. The function will return a character object with the most likely completion at each point in the sequence.

```{r, echo = FALSE, eval = FALSE}
food2 <- complete_prompt(prompt = 'My favorite food is', 
                max_tokens = 6)

save(food2, file = 'data/gpt-3-outputs/food2.RData')
```

```{r, echo = TRUE, eval = FALSE}
complete_prompt(prompt = 'My favorite food is', 
                max_tokens = 6)
```

```{r, echo = FALSE, eval = TRUE}
load('data/gpt-3-outputs/food2.RData')

food2
```

If we want thr LLM to perform a classification task, we can structure the prompt so that the best next-word prediction is the classification we want (a process called **adaptation**). Consider, for example, the following prompt.

```{r}
prompt <- 'Decide whether the following statement is happy, sad, or neither.\n\nText: I feel happy.\nClassification:'

cat(prompt)
```

```{r, echo = TRUE, eval = FALSE}
complete_prompt(prompt)
```

```{r, echo = FALSE, eval = FALSE}
prompt <- 'Decide whether the following statement is happy, sad, or neither.\n\nText: I feel happy.\nClassification:'

cat(prompt)

cat1 <- complete_prompt(prompt)

save(cat1, file = 'data/gpt-3-outputs/cat1.RData')
```

```{r, echo = FALSE, eval = TRUE}
load('data/gpt-3-outputs/cat1.RData')

cat1
```

The result is a probability vector with the most likely completions. The correct classification -- happy -- is assigned the highest probability, though note that some post-processing is necessary to combine the "happy" and "Happy" responses.

A **few-shot prompt** includes several completed examples in the text of the prompt to demonstrate the desired output. Prompts structured this way tend perform significantly better than zero-shot prompts with no examples [@brownLanguageModelsAre2020].

```{r}
prompt <- 'Decide whether the following statement is happy, sad, or neither.\n\nText: What should we do today?.\nClassification: Neither\n\nText: My puppy is so cute today.\nClassification: Happy\n\nText: The news is bumming me out.\nClassification: Sad\n\nText: I feel happy.\nClassification:'

cat(prompt)
```

```{r, echo = TRUE, eval = FALSE}
complete_prompt(prompt)
```

```{r, echo = FALSE, eval = FALSE}
cat2 <- complete_prompt(prompt)

save(cat2, file = 'data/gpt-3-outputs/cat2.RData')
```

```{r, echo = FALSE, eval=TRUE}
load('data/gpt-3-outputs/cat2.RData')

cat2
```

## Formatting Prompts

Manually typing prompts with multiple few-shot examples can be tedious and error-prone, particularly when performing the sort of context-specific prompting we recommend in our paper [@ornsteinHowTrainYour2025]. The `format_prompt()` function is a useful tool to aid in that process.

The function is designed with classification problems in mind. If you input the text you would like to classify along with a set of instructions, the default prompt template looks like this:

```{r}
library(promptr)

format_prompt(text = 'I am feeling happy today.',
              instructions = 'Decide whether this statment is happy, sad, or neither.')
```

You can customize the template using `glue` syntax, with placeholders for the text you want to classify {text} and the desired label {label}.

```{r}
format_prompt(text = 'I am feeling happy today.',
              instructions = 'Decide whether this statment is happy or sad.',
              template = 'Statement: {text}\nSentiment: {label}')
```

This is particularly useful when including few-shot examples in the prompt. If you input these examples as a tidy dataframe with the columns `text` and `label`, the `format_prompt()` function will paste them into the prompt them according to the template. To illustrate, let's classify the sentiment of a set of tweets about the Supreme Court of the United States, a dataset which is included with the `promptr` package.

## Classifying Documents

```{r}
data(scotus_tweets) # the full dataset
data(scotus_tweets_examples) # a sample of labeled examples
```

We can format our few-shot prompt template using `format_prompt()`, leaving the `{TWEET}` placeholder for the text we want to classify:

```{r}
library(tidyverse)
library(glue)

prompt <- format_prompt(text = '{TWEET}',
              instructions = 'Classify the sentiment of these tweets as Positive, Neutral, or Negative.',
              examples = scotus_tweets_examples |> 
                filter(case == 'masterpiece'),
              template = 'Tweet: {text}\nSentiment: {label}')

prompt
```

Using the `glue()` function, we can insert tweets into that template.

```{r}
TWEET <- scotus_tweets$text[42]
TWEET

glue(prompt)
```

We can then pipe that prompt into `complete_prompt()` to output the desired classification:

```{r, echo = TRUE, eval = FALSE}
prompt |> 
  glue() |> 
  complete_prompt()
```

```{r, echo = FALSE, eval = FALSE}
scotus1 <- prompt |> 
  glue() |> 
  complete_prompt()

save(scotus1, file = 'data/gpt-3-outputs/scotus1.RData')
```

```{r, echo = FALSE, eval = TRUE}
load('data/gpt-3-outputs/scotus1.RData')

scotus1
```

## Classifiation Performance

As we saw in the [sentiment analysis](sentiment-analysis.html) tutorial, conventional methods for classifying sentiment perform pretty poorly on text from social media. Does this approach perform any better? To find out, let's complete the prompt for each of the tweets in the `scotus_tweets` dataframe related to the Masterpiece Cakeshop ruling, create a measure of sentiment, and compare it against the human coders.

First, create our prompt template with `format_prompt()`, this time with a bit more detail in the instructions. For few-shot examples, we'll use the 'masterpiece' tweets in `scotus_tweets_examples`. These are six tweets that were unanimously coded by three human annotators as Positive, Negative, or Neutral (two per category).

```{r, echo = TRUE, eval = FALSE}
prompt <- format_prompt(text = '{TWEET}',
              instructions = 'Read these tweets posted the day after the US Supreme Court ruled in 
              favor of a baker who refused to bake a wedding cake for a same-sex couple. 
              For each tweet, decide whether its sentiment is Positive, Neutral, or Negative.',
              examples = scotus_tweets_examples |> 
                filter(case == 'masterpiece'),
              template = 'Tweet: {text}\nSentiment: {label}')

cat(prompt)
```

```{r, echo = FALSE, eval = TRUE}
prompt <- format_prompt(text = '{TWEET}',
              instructions = 'Read these tweets posted the day after the US Supreme Court ruled in favor of a baker who refused to bake a wedding cake for a same-sex couple. For each tweet, decide whether its sentiment is Positive, Neutral, or Negative.',
              examples = scotus_tweets_examples |> 
                filter(case == 'masterpiece'),
              template = 'Tweet: {text}\nSentiment: {label}')

cat(prompt)
```

Next, we'll create a function that takes the dataframe of next-word predictions from GPT-3 and converts it into a measure of sentiment. For this tutorial, our measure will be $P(\text{positive}) - P(\text{negative})$.

```{r}
sentiment_score <- function(df){
  
  p_positive <- df |> 
    # put responses in all caps
    mutate(response = str_to_upper(token)) |> 
    # keep and sum probabilities assigned to "POS"
    filter(str_detect(response, 'POS')) |> 
    pull(probability) |> 
    sum()
  
  p_negative <- df |> 
    # put responses in all caps
    mutate(response = str_to_upper(token)) |> 
    # keep and sum probabilities assigned to "NEG"
    filter(str_detect(response, 'NEG')) |> 
    pull(probability) |> 
    sum()
  
  return(p_positive - p_negative)
}
```

We can add this function to the end of the pipeline we developed before.

```{r, echo = TRUE, eval = FALSE}
prompt |> 
  glue() |> 
  complete_prompt() |> 
  sentiment_score()
```

```{r, echo = FALSE, eval = FALSE}
full_pipe <- prompt |> 
  glue() |> 
  complete_prompt() |> 
  sentiment_score()

save(full_pipe, file = 'data/gpt-3-outputs/full_pipe.RData')
```

```{r, echo = FALSE, eval=TRUE}
load('data/gpt-3-outputs/full_pipe.RData')

full_pipe
```

With these elements in place, let's loop through the `scotus_tweets` dataset and estimate sentiment scores for each tweet referencing Masterpiece Cakeshop. **Be mindful before you run this code.** At [current prices](https://openai.com/api/pricing/), OpenAI will charge \$1.50 per million input toikens, which works out to approximately 1/30 of a cent per prompt, for a total of $0.30 if you classify all 945 tweets.

Let's create a formatted few-shot prompt for each tweet about the Masterpiece Cakeshop decision, and return the sentiment score for each.

```{r, echo = TRUE, eval = FALSE}
masterpiece_tweets <- scotus_tweets |> 
  filter(case == 'masterpiece')

instructions <- 'Read these tweets posted the day after the US Supreme Court ruled in favor of a baker who refused to bake a wedding cake for a same-sex couple (Masterpiece Cakeshop, 2018). For each tweet, decide whether its sentiment is Positive, Neutral, or Negative.'

masterpiece_examples <- scotus_tweets_examples |> 
  filter(case == 'masterpiece')

masterpiece_tweets$prompt <- format_prompt(text = masterpiece_tweets$text,
                                           instructions = instructions,
                                           examples = masterpiece_examples)

masterpiece_tweets$prompt[3]
```

Now submit the entire list of prompts to `complete_prompt()`:

```{r echo = TRUE, eval = FALSE}
masterpiece_tweets$out <- complete_prompt(masterpiece_tweets$prompt)
```

```{r echo = FALSE, eval = FALSE}
masterpiece_tweets$out <- NA
masterpiece_tweets$out[1:200] <- complete_prompt(masterpiece_tweets$prompt[1:200])
masterpiece_tweets$out[201:423] <- complete_prompt(masterpiece_tweets$prompt[201:423])

save(masterpiece_tweets, file = 'data/gpt-3-outputs/masterpiece.RData')
```

```{r echo = FALSE, eval = TRUE}
load('data/gpt-3-outputs/masterpiece.RData')
```

The estimated probability distribution for each completion is now a list of dataframes in the `out` column. We can compute our sentiment score for each one:

```{r}
masterpiece_tweets$score <- masterpiece_tweets$out |> 
  lapply(mutate, token = str_to_lower(token)) |> 
  lapply(summarize, 
         positive = sum(probability[token=='positive']), 
         negative = sum(probability[token=='negative'])) |>
  lapply(summarize,score=positive-negative) |> 
  unlist()
```

Plotting those sentiment scores against the average of the hand-coded scores, we can see that this measure is much better than the one from the [dictionary method](sentiment-analysis.html) we tried before. The correlation between the two scores is `r cor((masterpiece_tweets$expert1 + masterpiece_tweets$expert2 + masterpiece_tweets$expert3) / 3, masterpiece_tweets$score, use = 'pairwise.complete.obs') |> round(2)`.

```{r}
ggplot(data = masterpiece_tweets,
       mapping = aes(x = (expert1 + expert2 + expert3) / 3,
                     y = score)) +
  geom_jitter(width = 0.1) +
  labs(x = 'Hand-Coded Sentiment Score',
       y = 'GPT-3.5 Sentiment Score') +
  theme_minimal()
```

All in all, few-shot prompting an LLM is a powerful method for classifying documents without the need for extensive fine-tuning or large sets of training data, as with [supervised learning](supervised-learning.html) methods.

## Cleaning Up OCR

In the [OCR tutorial](OCR.html), we worked with a newspaper clipping about the sinking of the Titanic.

```{r}
library(tesseract)
library(magick)
library(promptr)

image <- image_read('img/titanic.png')
image
```

Let's focus for now on the first column.

```{r}
first_column <- image_crop(image,
                           geometry = '336 x 660 + 0 + 0')

first_column
```

Though a human can easily read and interpret this text, converting the image to plain text is a non-trivial problem in the field of computer vision. Note that the text is slightly tilted in places, and some lines are squished or cut off. A smudge obscures some letters in the lower left corner.

How well does OCR capture the text from that image?

```{r}
text <- ocr(first_column)

cat(text)
```

As is common with OCR, the result is generally quite good, but not perfect. Notice, for example, the phrases "reused tu recusiiee ce", "lif¢-boats", and "Ti-,'sanic's". Look closely at the letters in the image above to see how the OCR algorithm may have made those mistakes. When you consider the image letter-by-letter, ignoring the semantic context in which those letters are placed, it is easy to mistake some letters for others. But when humans read a passage like this, they're not reading it letter-by-letter. Instead, fluent readers learn to recognize whole words at a time, using their knowledge of the language to "predict" what the constituent letters of a word must be, even when they are difficult to make out on the page. In the same way, we can make use of the fact that LLMs are very good at predicting words in sequences to impute what the phrase "reused tu recusiiee ce" is likely to have been, given the context.

To do so, we can prompt the LLM like so:

```{r}
prompt <- glue('Correct OCR errors in the following passage.\n---\nOriginal Passage:\n\n{text}\n---\nCorrected Passage:\n\n')

cat(prompt)
```

```{r, echo = TRUE, eval = FALSE}
cleaned_text <- complete_prompt(
  prompt = prompt,
  max_tokens = nchar(text))

cat(cleaned_text)
```

```{r, echo = FALSE, eval = FALSE}
cleaned_text <- complete_prompt(
  prompt = prompt,
  max_tokens = nchar(text))

save(cleaned_text, file = 'data/gpt-3-outputs/titanic_clean.RData')
```

```{r, echo = FALSE, eval = TRUE}
load('data/gpt-3-outputs/titanic_clean.RData')

cat(cleaned_text)
```

The resulting text is a nearly perfect transcription, despite the garbled inputs!

## Text To Data

One of the most labor-intensive tasks in a research workflow involves converting unstructured text to structured datasets. Traditionally, this is a task that has only been suitable for human research assistants, but with LLMs, a truly automated workflow is feasible. Consider the following prompt:

```{r, echo = FALSE}
prompt <- 'Create a data table from the following passage.\n---\nThe little dog laughed to see such fun, and the dish ran away with the spoon.\n---\nData Table:\nCharacter | What They Did \n---|---\n'

cat(prompt)
```

When we submit this prompt to ChatGPT, it yields a data table delimited by vertical bars, which can then be read into a dataframe.

```{r, echo = TRUE, eval = FALSE}
prompt <- 'Create a data table from the following passage.\n---\nThe little dog laughed to see such fun, and the dish ran away with the spoon.\n---\nData Table:\nCharacter | What They Did \n---|---\n'

response <- complete_prompt(prompt, 
                            max_tokens = 100)

response
```

```{r, echo = FALSE, eval = FALSE}
save(response, file = 'data/gpt-3-outputs/parse1.RData')
```

```{r, echo = FALSE, eval = TRUE}
load('data/gpt-3-outputs/parse1.RData')
response
```

```{r}
df <- read_delim(response,
                 delim = '|',
                 col_names = FALSE)

df
```

## Practice Problems

1.  Take a sample of the Senate press releases from the [clustering tutorial](clustering.html). Format a few-shot ChatGPT prompt that returns a list of topics. Are the topic labels sensible? Are they similar to what we came up with using unsupervised methods?

2.  Ask ChatGPT to summarize the remarks on pages 4-5 of the [PDF](img/SOJ.pdf) that you imported in the [OCR practice problems](OCR.html).

## Further Reading

- Ornstein, Blasingame, and Truscott (2025). "How to Train Your Stochastic Parrot: Large Language Models for Political Texts". *Political Science Research & Methods*. [Link](https://www.cambridge.org/core/journals/political-science-research-and-methods/article/how-to-train-your-stochastic-parrot-large-language-models-for-political-texts/8EAA8096779B5C9DDBDF1BFC71C63AEC)