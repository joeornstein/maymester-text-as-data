---
title: "Sentiment Analysis"
description: |
  Teaching the computer to understand feelings.
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

For unsupervised learning models like [k-means](clustering.html) and [LDA](LDA.html), the objective is often discovery. We have a set of unlabeled data, and we want to get a sense of how we might organize the documents, how we might sort them into buckets. But frequently social scientists turn to text data because we're interested in *measuring* some concept that is tough to quantify in other ways. For this, we'll want a different set of tools, particularly supervised learning methods, because we have an objective and we want to build a model that satisfies that objective.

To illustrate the process of measurement using text data, let's consider the field of **sentiment analysis**. We have a set of documents, and we're interested in classifying the sentiment/emotion the author is trying to convey -- positive, negative, or neutral. Here is a dataset of 945 tweets about the Supreme Court that I compiled for a project with Jake Truscott and Elise Blasingame.

```{r}
library(tidyverse)
library(tidytext)

# load the tweets
tweets <- read_csv('data/supreme-court-tweets.csv')

tweets |> select(-tweet_id)
```

## Hand Coding

The dataset contains the text of the tweet, plus three "expert" ratings on a scale from -1 (negative), 0 (neutral), to 1 (positive). Each author independently read and coded each tweet[^1] then discussed the cases where we disagreed, going back for a second round of coding on those tweets where everyone produced a different measure. One way to assess how well we did at capturing sentiment is inter-coder reliability (aka [Fleiss' kappa](https://en.wikipedia.org/wiki/Fleiss%27_kappa)), which measures how frequently the coders agreed relative to chance, on a scale from -1 (complete disagreement on everything) to 1 (perfect agreement on everything).

[^1]: Yes, it was dreadful and I don't recommend trying this at home.

```{r}
library(irr)
tweets |>
  select(expert1, expert2, expert3) |>
  kappam.fleiss()
```

## Dictionary Classification

UNDER CONSTRUCTION (SEE NOTES)

## Up Next: Supervised Learning

Both human-coding and dictionary classification are examples of *rule-based* measurement. You decide in advance exactly what steps you will take to measure each document, and then you (or your computer) follow the rules you set out. The problem with such rules-based measures is that they are either:

1.  Not scalable (e.g. human-coding). For a dataset of 945 tweets, we were able to tackle it in relatively short order. But if we were interested in 100,000 tweets? Or a million tweets? There's no way to scale that procedure, except with crowd coding on something like Amazon's MTurk [@benoit2016; @carlson2017], and that gets expensive quickly.
2.  Scalable, but terrible (e.g. dictionary methods). With dictionary methods, it's trivial to classify a million tweets. But the results, as we have seen, are fairly crummy.

The best alternative to rules-based classification is *statistical* classification, and that is the topic of the [next page](supervised-learning.html).

## Practice Problems

1.  How accurate does the dictionary classifier need to be until it's "good enough"? A useful benchmark is to compare your model against a *null model*. For example, in the Twitter corpus, how accurate is the null model "predict every tweet will be negative"?
2.  How accurate can you get the dictionary classifier to be, by varying the lexicon and modifying the word list to match our specific context (i.e. filtering out words whose dictionary meaning and context-specific meaning are different, like "Trump" and "Supreme")?

## Further Reading
